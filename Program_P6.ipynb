{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Program_P6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "icIiX96T_oHR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def clean_text(text):\n",
        "    ''' Lowering text and removing undesirable marks\n",
        "    '''\n",
        "    \n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"\\'\\n\", \" \", text)\n",
        "    text = re.sub(r\"\\'\\xa0\", \" \", text)\n",
        "    text = re.sub('\\s+', ' ', text) # matches all whitespace characters\n",
        "    text = text.strip(' ')\n",
        "    return text\n",
        "\n",
        "\n",
        "def strip_list_noempty(mylist):\n",
        "    \n",
        "    newlist = (item.strip() if hasattr(item, 'strip') else item for item in mylist)\n",
        "    return [item for item in newlist if item != '']\n",
        "\n",
        "\n",
        "def clean_punct(text): \n",
        "    \n",
        "    words = token.tokenize(text)\n",
        "    punctuation_filtered = []\n",
        "    regex = re.compile('[%s]' % re.escape(punct))\n",
        "    remove_punctuation = str.maketrans(' ', ' ', punct)\n",
        "    \n",
        "    for w in words:\n",
        "        if w in top_tags:\n",
        "            punctuation_filtered.append(w)\n",
        "        else:\n",
        "            w = re.sub('^[0-9]*', \" \", w)\n",
        "            punctuation_filtered.append(regex.sub('', w))\n",
        "  \n",
        "    filtered_list = strip_list_noempty(punctuation_filtered)\n",
        "        \n",
        "    return ' '.join(map(str, filtered_list))\n",
        "\n",
        "\n",
        "def stopWordsRemove(text):\n",
        "    ''' Removing all the english stop words from a corpus\n",
        "\n",
        "    Parameter:\n",
        "\n",
        "    text: corpus to remove stop words from it\n",
        "    '''\n",
        "\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    words = token.tokenize(text)\n",
        "    filtered = [w for w in words if not w in stop_words]\n",
        "    \n",
        "    return ' '.join(map(str, filtered))\n",
        "\n",
        "\n",
        "def lemmatization(texts, allowed_postags, stop_words=stop_words):\n",
        "    ''' It keeps the lemma of the words (lemma is the uninflected form of a word),\n",
        "    and deletes the underired POS tags\n",
        "    \n",
        "    Parameters:\n",
        "    \n",
        "    texts (list): text to lemmatize\n",
        "    allowed_postags (list): list of allowed postags, like NOUN, ADL, VERB, ADV\n",
        "    '''\n",
        "\n",
        "    nlp = spacy.load('en', disable=['parser', 'ner'])\n",
        "    lemma = wordnet.WordNetLemmatizer()       \n",
        "    doc = nlp(texts) \n",
        "    texts_out = []\n",
        "    \n",
        "    for token in doc:\n",
        "        \n",
        "        if str(token) in top_tags:\n",
        "            texts_out.append(str(token))\n",
        "            \n",
        "        elif token.pos_ in allowed_postags:\n",
        "            \n",
        "            if token.lemma_ not in ['-PRON-']:\n",
        "                texts_out.append(token.lemma_)\n",
        "                \n",
        "            else:\n",
        "                texts_out.append('')\n",
        "     \n",
        "    texts_out = ' '.join(texts_out)\n",
        "\n",
        "    return texts_out\n",
        "\n",
        "\n",
        "def Recommandation_system_tags(text):\n",
        "'''  '''\n",
        "    \n",
        "    !wget https://www.dropbox.com/s/zr6bpvt6lkr0ew7/P6_data.csv?dl=0\n",
        "\n",
        "    df = pd.read_csv('P6_data.csv?dl=0')\n",
        "        # Sampling dataset\n",
        "    vectorizer_X = TfidfVectorizer(analyzer='word', min_df=0.0, max_df = 1.0, \n",
        "                                    strip_accents = None, encoding = 'utf-8', \n",
        "                                    preprocessor=None, \n",
        "                                    token_pattern=r\"(?u)\\S\\S+\", # Need to repeat token pattern\n",
        "                                    max_features=1000)\n",
        "\n",
        "    # Binarizing the tags for the supervised models\n",
        "    multilabel_binarizer = MultiLabelBinarizer()\n",
        "    y_target = multilabel_binarizer.fit_transform(df['Tags'])\n",
        "\n",
        "    # 80/20 split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        df['Body'], y_target, test_size=0.2,train_size=0.8, random_state=0)\n",
        "\n",
        "    # TF-IDF matrices\n",
        "    X_tfidf_train = vectorizer_X.fit_transform(X_train)\n",
        "    X_tfidf_test = vectorizer_X.transform(X_test)\n",
        "\n",
        "    text = clean_text(text)\n",
        "    text = no_code(text)\n",
        "    text = clean_punct(text)\n",
        "    text = stopWordsRemove(text)\n",
        "    text = lemmatization(text, ['NOUN', 'ADV'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}